# O que é Apache Airflow

Apache Airflow™ é uma plataforma de código aberto para desenvolvimento, agendamento, e monitoramento de fluxos de trabalho orientados a lotes. A estrutura Python extensível do Airflow permite que você crie fluxos de trabalho conectando-se com praticamente qualquer tecnologia. Uma interface da Web ajuda a gerenciar o estado de seus fluxos de trabalho. O Airflow é Implantável de várias maneiras, variando de um único processo em seu laptop a uma configuração distribuída para suportar até mesmo os maiores fluxos de trabalho.

### Fluxos de trabalho como código

A principal característica dos fluxos de trabalho do Airflow é que todos os fluxos de trabalho são definidos em código Python. "Fluxos de trabalho como code" serve a vários propósitos:

* Dinâmico: Os pipelines de Airflow são configurados como código Python, permitindo a geração dinâmica de pipelines.
* Extensível: A estrutura de Airflow™ contém operadores para se conectar com várias tecnologias. Todos os componentes de Airflow são extensíveis para se ajustar facilmente ao seu ambiente.
* Flexível: a parametrização do fluxo de trabalho é integrada aproveitando o mecanismo de modelagem Jinja.

Dê uma olhada no seguinte trecho de código:
```
from datetime import datetime

from airflow import DAG
from airflow.decorators import task
from airflow.operators.bash import BashOperator

# A DAG represents a workflow, a collection of tasks
with DAG(dag_id="demo", start_date=datetime(2022, 1, 1), schedule="0 0 * * *") as dag:

    # Tasks are represented as operators
    hello = BashOperator(task_id="hello", bash_command="echo hello")

    @task()
    def airflow():
        print("airflow")

    # Set dependencies between tasks
    hello >> airflow()
```

### Por que o Airflow?

O Airflow é uma plataforma de orquestração de fluxo de trabalho em lote. A estrutura de Airflow contém operadores com os quais se conectar muitas tecnologias e é facilmente extensível para se conectar com uma nova tecnologia. Se os seus fluxos de trabalho tiverem um iniciar e terminar, e executar em intervalos regulares, eles podem ser programados como um DAG de Airflow.

Se você prefere codificar a clicar, o Airflow é a ferramenta para você. Os fluxos de trabalho são definidos como código Python que significa:

* Os fluxos de trabalho podem ser armazenados no controle de versão para que você possa reverter para versões anteriores
* Os fluxos de trabalho podem ser desenvolvidos por várias pessoas simultaneamente
* Os testes podem ser escritos para validar a funcionalidade
* Os componentes são extensíveis e você pode criar uma ampla coleção de componentes existentes


A semântica avançada de agendamento e execução permite que você defina facilmente pipelines complexos, executados regularmente Intervalos. O preenchimento permite que você (re)execute pipelines em dados históricos depois de fazer alterações em sua lógica. E a capacidade de executar novamente pipelines parciais depois de resolver um erro ajuda a maximizar a eficiência.

A interface de usuário do Airflow fornece:

1. Visões detalhadas de duas coisas:
   1. Pipelines
   2. Tarefas
2. Visão geral de seus pipelines ao longo do tempo

Na interface, você pode inspecionar logs e gerenciar tarefas, por exemplo, repetir uma tarefa em caso de falha.

A natureza de código aberto do Airflow garante que você trabalhe em componentes desenvolvidos, testados e usados por muitas outras empresas ao redor do mundo. Na comunidade ativa você pode encontrar muitos recursos úteis na forma de postagens em blogs, artigos, conferências, livros e muito mais. Você pode se conectar com outros colegas através de vários canais como Slack e listas de discussão.

### Por que não Airflow?

O Airflow foi criado para fluxos de trabalho em lote finito. Embora a CLI e a API REST permitam o acionamento de fluxos de trabalho, o Airflow não foi criado para fluxos de trabalho baseados em eventos em execução infinita. O Airflow não é uma solução de streaming. No entanto, um sistema de streaming como o Apache Kafka é frequentemente visto trabalhando em conjunto com o Apache Airflow. O Kafka pode ser usado para ingestão e processamento em tempo real, os dados do evento são gravados em um local de armazenamento e o Airflow inicia periodicamente um fluxo de trabalho processando um lote de dados.
