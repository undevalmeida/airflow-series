# O que é Apache Airflow

Apache Airflow™ é uma plataforma de código aberto para desenvolvimento, agendamento, e monitoramento de fluxos de trabalho orientados a lotes. A estrutura Python extensível do Airflow permite que você crie fluxos de trabalho conectando-se com praticamente qualquer tecnologia. Uma interface da Web ajuda a gerenciar o estado de seus fluxos de trabalho. O fluxo de ar é Implantável de várias maneiras, variando de um único processo em seu laptop a uma configuração distribuída para suportar até mesmo os maiores fluxos de trabalho.

### Fluxos de trabalho como código

A principal característica dos fluxos de trabalho do Airflow é que todos os fluxos de trabalho são definidos em código Python. "Fluxos de trabalho como code" serve a vários propósitos:

* Dinâmico: Os pipelines de fluxo de ar são configurados como código Python, permitindo a geração dinâmica de pipelines.
* Extensível: A estrutura de fluxo de ar™ contém operadores para se conectar com várias tecnologias. Todos os componentes de fluxo de ar são extensíveis para se ajustar facilmente ao seu ambiente.
* Flexível: a parametrização do fluxo de trabalho é integrada aproveitando o mecanismo de modelagem Jinja.

Dê uma olhada no seguinte trecho de código:
```
from datetime import datetime

from airflow import DAG
from airflow.decorators import task
from airflow.operators.bash import BashOperator

# A DAG represents a workflow, a collection of tasks
with DAG(dag_id="demo", start_date=datetime(2022, 1, 1), schedule="0 0 * * *") as dag:

    # Tasks are represented as operators
    hello = BashOperator(task_id="hello", bash_command="echo hello")

    @task()
    def airflow():
        print("airflow")

    # Set dependencies between tasks
    hello >> airflow()
```